{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "***\n",
    "* Assigned: 02/07\n",
    "* Due: **02/27 at 11:59pm** electronically\n",
    "* This assignment is worth 100 points.\n",
    "\n",
    "**Please do read the instructions for each response carefully, since a whole lot of different variety of reponses are involved in the assignment (including pasting code snippets, writing executable code, small writeups). You don't wanna be losing points for silly errors.**\n",
    "\n",
    "### Jupyter Notes:\n",
    "\n",
    "* You **may** create new IPython notebook cells to use for e.g. testing, debugging, exploring, etc.- this is encouraged in fact!\n",
    "  * you can press shift+enter to execute the code in the cell that your cursor is in.\n",
    "* When you see `In [*]:` to the left of the cell you are executing, this means that the code / query is _running_. Please wait for the execution to complete\n",
    "    * **If the cell is hanging- i.e. running for too long: you can restart the kernel**\n",
    "    * To restart kernel using the menu bar: \"Kernel >> Restart >> Clear all outputs & restart\"), then re-execute cells from the top\n",
    "* _Have fun!_\n",
    "\n",
    "\n",
    "### Setup Your Credentials\n",
    "\n",
    "Update the following variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your columbia uni that is used in SSOL\n",
    "#\n",
    "# IMPORTANT:  make sure this is consistent with the uni/alias used as your @columbia.edu email in SSOL\n",
    "#\n",
    "UNI = \"jl4961\"\n",
    "\n",
    "# your instabase username (if you go to the instabase homepage, your username should be in the URL)\n",
    "USER = \"jl4961\"\n",
    "\n",
    "# your repository name containing \n",
    "REPO = \"my-repo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "\n",
    "In this lab, you will use various types of tools -- from high-level tools like Data Wrangler to command line tools like `sed` and `awk` -- to perform data parsing and extraction from data encoded into a text file.  The goal of this lab is simply to gain experience with these tools and compare and contrast their usage.\n",
    "\n",
    "The `lab` directory contains two datasets that you will work with:\n",
    "\n",
    "1. A dataset of all the movies in 2013 from January to March (`2013films.txt`). It contains Movie name, Production house, Genre, Publisher and some other details.\n",
    "\n",
    "1. The second dataset (`worldcup.txt`) is a snippet of the following Wikipedia webpage on [FIFA (Soccer) World Cup](http://en.wikipedia.org/wiki/FIFA_World_Cup).\n",
    "Specifically it is a partially cleaned-up wiki source for the table toward the end of the page that lists teams finishing in the top 4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 1. Get Wrangling !!\n",
    "***\n",
    "Download the [Trifacta Wrangler](https://www.trifacta.com/products/wrangler/) tool. Load both the datasets into wrangler and try playing around with the tool.\n",
    "\n",
    "Some tips using Wrangler:\n",
    "\n",
    "* Check out the introduction [video](https://vimeo.com/19185801) to get a feel of how wrangler works.\n",
    "* You may wanna start off by loading ~50 lines of data instead of the entire files and play around with the tool.\n",
    "* Wrangler responds to mouse highlights and clicks on the displayed table cells by suggesting operations on the left sidebar.  \n",
    "* Hovering over each element shows the result in the table view.  \n",
    "* Clicking adds the operation.  \n",
    "* Clear the sidebar by clicking the colored row above the schema row.\n",
    "\n",
    "## Tasks:\n",
    "\n",
    "Use Data Wrangler for the following two datasets.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2013films.txt\n",
    "\n",
    "Use the tool to generate output as follows, i.e., Movie name, Production/Distribtuion house, Director, Genre and publisher. \n",
    "\n",
    "        'A dark truth, Magnolia Pictures, Damian Lee, Action,ComingSoon.net\n",
    "         Table No. 21, Eros International, Aditya Datt, Thriller, BoxOfficeMojo.com\n",
    "         ...\n",
    "        \n",
    "For the purpose of explanation columns are separated by ||. You can choose any pattern to extract information. \n",
    "\n",
    "1. Movie name can be identified as first column in every line formatted as ''[[ <movie name> ]]''  \n",
    "1. Production/Distribution house is the following column that is formatted as [[< Production house>]]  \n",
    "1. Director name can be identified with \"(director)\" tag that follows the name  \n",
    "1. Genre is present in the next column but make sure to extract only second part that is separated by | operator. For eg. in [Action film|Action] relevant information is Action  \n",
    "1. Publisher name can be identified in the last column with format \"publisher=<publisher name>\"  \n",
    "1. It may help to skip first few lines that contains html code, so that you process actual records.\n",
    "\n",
    "#### Questions\n",
    "\n",
    "* Use wrangler to clean the data, then determine which Production/Distribution house produced maximum movies.\n",
    "\n",
    "#### Notes\n",
    "* Your wrangler script is not expected to be sophisticated enough to generate the results for the question asked. It just needs to clean/combine the data enough for you to observe the data and answer the question.\n",
    "* You can export the transformations you carried out in wrangler.\n",
    "Export the script and paste it in the cell below. Do not bother executing it here in the notebook.\n",
    "* Stanford also has an online open-source version of Trifacta Wrangler that can be used [here](http://vis.stanford.edu/wrangler/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your wrangler script goes in this cell below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### worldcup.txt\n",
    "\n",
    "Use the tool to generate output as follows, i.e., each line in the output contains a country, a year, and the position of the county in that year (if within top 4).\n",
    "\n",
    "        BRA, 1962, 1\n",
    "        BRA, 1970, 1\n",
    "        BRA, 1994, 1\n",
    "        BRA, 2002, 1\n",
    "        BRA, 1958, 1\n",
    "        BRA, 1998, 2\n",
    "        BRA, 1950, 2\n",
    "        ...\n",
    "\n",
    "It may help to \n",
    "\n",
    "1. Skip the first 20 or so rows of table headers and other text, so that the data wrangler works with are \"record text\".  \n",
    "2. Delete the rows that are clearly HTML formatting content\n",
    "3. Extract the relevant data from the remaining column into new columns\n",
    "4. Use the fill operation\n",
    "\n",
    "#### Questions\n",
    "\n",
    "* According to the dataset, how often has each country won the world cup?\n",
    "\n",
    "#### Notes\n",
    "* Your wrangler script is not expected to be sophisticated enough to generate the results for the question asked. It just needs to clean/combine the data enough for you to observe the data and answer the question.\n",
    "* You can export the transformations you carried out in wrangler. Export the script and paste it in the cell below. Do not bother executing it here in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your wrangler script goes in this cell below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 2. Grep, Sed & Awk\n",
    "***\n",
    "\n",
    "The set of three UNIX tools, sed, awk, and grep, can be very useful for quickly cleaning up and transforming data for further analysis (and have been around since the inception of UNIX). In conjunction with other unix utilities like sort, uniq, tail, head, etc., you can accomplish many simple data parsing and cleaning tasks with these tools. You are encouraged to play with these tools and familiarize yourselves with the basic usage of these tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# grep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "grep 'regexp' filename\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or equivalently (using UNIX pipelining):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```bash\n",
    "cat filename | grep 'regexp'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output contains only those lines from the file that match the regular expression. Two options to grep are useful: grep -v will output those lines that do not match the regular expression, and grep -i will ignore case while matching. See the manual (man grep) (or online resources) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sed stands for stream editor. Basic syntax for sed is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```bash\n",
    "sed 's/regexp/replacement/g' filename\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each line in the intput, the portion of the line that matches regexp (if any) is replaced with replacement. Sed is quite powerful within the limits of operating on single line at a time. You can use \\( \\) to refer to parts of the pattern match. In the first sed command above, the sub-expression within \\( \\) extracts the user id, which is available to be used in the replacement as \\1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# awk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, awk is a powerful scripting language (not unlike perl). The basic syntax of awk is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```bash\n",
    "awk -F',' 'BEGIN{commands} /regexp1/ {command1} /regexp2/ {command2} END{commands}'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each line, the regular expressions are matched in order, and if there is a match, the corresponding command is executed (multiple commands may be executed for the same line). BEGIN and END are both optional. The -F',' specifies that the lines should be split into fields using the separator \",\", and those fields are available to the regular expressions and the commands as $1, $2, etc. See the manual (man awk) or online resources for further details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXAMPLES\n",
    "\n",
    "#### Note\n",
    "There's nothing to submit in the examples section. Task to carry out and submit will follow after this section. Just play around and get a hang of the bash commands described here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "We start off by copying the files from our instabase repository to the VM filesystem our instabse instance is running on.\n",
    "Remember, you'll have to execute the cell below everytime the VM is restarted (happens when you close and restart the notebook) before you can proceed with the bash examples that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ib.open(\"labor.csv\") as f:\n",
    "    labor=f.read()\n",
    "with ib.open(\"crime-clean.txt\") as f:\n",
    "    crime_clean=f.read()\n",
    "with ib.open(\"crime-unclean.txt\") as f:\n",
    "    crime_unclean=f.read()\n",
    "\n",
    "open('/tmp/labor.csv','w').write(labor)\n",
    "open('/tmp/crime-clean.txt','w').write(crime_clean)\n",
    "open('/tmp/crime_unclean.txt','w').write(crime_unclean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few examples to give you a flavor of the tools and what one can do with them.\n",
    "\n",
    "* Perform the equivalent of wrap on labor.csv (i.e., merge consecutive groups of lines referring to the same record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Series Id: LNU02000000, Series title: (Unadj) Employment Level, Labor force status: Employed, Type of data: Number in thousands, Age: 16 years and over\n",
      "Series Id: LNU03000000, Series title: (Unadj) Unemployment Level, Labor force status: Unemployed, Type of data: Number in thousands, Age: 16 years and over\n",
      "Series Id: LNU04000000, Series title: (Unadj) Unemployment Level, Labor force status: Unemployed, Type of data: Number in thousands, Age: 16 years and over\n",
      "Series Id: LNU02032201, Series title: (Unadj) Employment Level - Management, Professional, and Related Occupations, Labor force status: Employed, Type of data: Number in thousands, Age: 16 years and over, Occupation: Management, professional, and related occupations (0008-3540)\n",
      "Series Id: LNU05000000, Series title: (Unadj) Unemployment Level, Labor force status: Unemployed, Type of data: Number in thousands, Age: 16 years and over\n",
      "Series Id: LNU03032215, Series title: (Unadj) Unemployment Level - Management, Professional, and Related Occupations, Labor force status: Unemployed, Type of data: Number in thousands, Age: 16 years and over, Labor force experience: Experienced, Occupation: Management, professional, and related occupations (0008-3540)\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "cat /tmp/labor.csv | awk '/^Series Id:/ {print combined; combined = $0} \n",
    "                    !/^Series Id:/ {combined = combined\", \"$0;}\n",
    "                    END {print combined}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note that all bash cells begin with **%%bash**. This indicates that, what follows is a bash code/script.*\n",
    "\n",
    "\n",
    "* On crime-clean.txt, the following command does a fill (first row of output: \"Alabama, 2004, 4029.3\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alabama, 2004,4029.3\n",
      "Alabama, 2005,3900\n",
      "Alabama, 2006,3937\n",
      "Alabama, 2007,3974.9\n",
      "Alabama, 2008,4081.9\n",
      "Alaska, 2004,3370.9\n",
      "Alaska, 2005,3615\n",
      "Alaska, 2006,3582\n",
      "Alaska, 2007,3373.9\n",
      "Alaska, 2008,2928.3\n",
      "Arizona, 2004,5073.3\n",
      "Arizona, 2005,4827\n",
      "Arizona, 2006,4741.6\n",
      "Arizona, 2007,4502.6\n",
      "Arizona, 2008,4087.3\n",
      "Arkansas, 2004,4033.1\n",
      "Arkansas, 2005,4068\n",
      "Arkansas, 2006,4021.6\n",
      "Arkansas, 2007,3945.5\n",
      "Arkansas, 2008,3843.7\n",
      "California, 2004,3423.9\n",
      "California, 2005,3321\n",
      "California, 2006,3175.2\n",
      "California, 2007,3032.6\n",
      "California, 2008,2940.3\n",
      "Colorado, 2004,3918.5\n",
      "Colorado, 2005,4041\n",
      "Colorado, 2006,3441.8\n",
      "Colorado, 2007,2991.3\n",
      "Colorado, 2008,2856.7\n",
      "Connecticut, 2004,2684.9\n",
      "Connecticut, 2005,2579\n",
      "Connecticut, 2006,2575\n",
      "Connecticut, 2007,2470.6\n",
      "Connecticut, 2008,2490.8\n",
      "Delaware, 2004,3283.6\n",
      "Delaware, 2005,3118\n",
      "Delaware, 2006,3474.5\n",
      "Delaware, 2007,3427.1\n",
      "Delaware, 2008,3594.7\n",
      "District 2004,4852.8\n",
      "District 2005,4490\n",
      "District 2006,4653.9\n",
      "District 2007,4916.3\n",
      "District 2008,5104.6\n",
      "Florida, 2004,4182.5\n",
      "Florida, 2005,4013\n",
      "Florida, 2006,3986.2\n",
      "Florida, 2007,4088.8\n",
      "Florida, 2008,4140.6\n",
      "Georgia, 2004,4223.5\n",
      "Georgia, 2005,4145\n",
      "Georgia, 2006,3928.8\n",
      "Georgia, 2007,3893.1\n",
      "Georgia, 2008,3996.6\n",
      "Hawaii, 2004,4795.5\n",
      "Hawaii, 2005,4800\n",
      "Hawaii, 2006,4219.9\n",
      "Hawaii, 2007,4119.3\n",
      "Hawaii, 2008,3566.5\n",
      "Idaho, 2004,2781\n",
      "Idaho, 2005,2697\n",
      "Idaho, 2006,2386.9\n",
      "Idaho, 2007,2264.2\n",
      "Idaho, 2008,2116.5\n",
      "Illinois, 2004,3174.1\n",
      "Illinois, 2005,3092\n",
      "Illinois, 2006,3019.6\n",
      "Illinois, 2007,2935.8\n",
      "Illinois, 2008,2932.6\n",
      "Indiana, 2004,3403.6\n",
      "Indiana, 2005,3460\n",
      "Indiana, 2006,3464.3\n",
      "Indiana, 2007,3386.5\n",
      "Indiana, 2008,3339.6\n",
      "Iowa, 2004,2904.8\n",
      "Iowa, 2005,2845\n",
      "Iowa, 2006,2870.3\n",
      "Iowa, 2007,2648.6\n",
      "Iowa, 2008,2440.5\n",
      "Kansas, 2004,4015.5\n",
      "Kansas, 2005,3806\n",
      "Kansas, 2006,3858.5\n",
      "Kansas, 2007,3693.8\n",
      "Kansas, 2008,3397\n",
      "Kentucky, 2004,2540.2\n",
      "Kentucky, 2005,2531\n",
      "Kentucky, 2006,2621.9\n",
      "Kentucky, 2007,2524.6\n",
      "Kentucky, 2008,2677.1\n",
      "Louisiana, 2004,4419.1\n",
      "Louisiana, 2005,3696\n",
      "Louisiana, 2006,4088.5\n",
      "Louisiana, 2007,4196.1\n",
      "Louisiana, 2008,3880.2\n",
      "Maine, 2004,2413.7\n",
      "Maine, 2005,2419\n",
      "Maine, 2006,2546.1\n",
      "Maine, 2007,2448.3\n",
      "Maine, 2008,2463.7\n",
      "Maryland, 2004,3640.7\n",
      "Maryland, 2005,3551\n",
      "Maryland, 2006,3481.2\n",
      "Maryland, 2007,3431.5\n",
      "Maryland, 2008,3516\n",
      "Massachusetts, 2004,2468.2\n",
      "Massachusetts, 2005,2358\n",
      "Massachusetts, 2006,2396\n",
      "Massachusetts, 2007,2399.2\n",
      "Massachusetts, 2008,2402\n",
      "Michigan, 2004,3066.1\n",
      "Michigan, 2005,3098\n",
      "Michigan, 2006,3226\n",
      "Michigan, 2007,3057.8\n",
      "Michigan, 2008,2945.7\n",
      "Minnesota, 2004,3041.6\n",
      "Minnesota, 2005,3088\n",
      "Minnesota, 2006,3088.8\n",
      "Minnesota, 2007,3045\n",
      "Minnesota, 2008,2858.1\n",
      "Mississippi, 2004,3481.1\n",
      "Mississippi, 2005,3274\n",
      "Mississippi, 2006,3213\n",
      "Mississippi, 2007,3137.8\n",
      "Mississippi, 2008,2941.7\n",
      "Missouri, 2004,3900.1\n",
      "Missouri, 2005,3929\n",
      "Missouri, 2006,3828.4\n",
      "Missouri, 2007,3828.2\n",
      "Missouri, 2008,3663.6\n",
      "Montana, 2004,2936.1\n",
      "Montana, 2005,3146\n",
      "Montana, 2006,2863.4\n",
      "Montana, 2007,2863.6\n",
      "Montana, 2008,2720.9\n",
      "Nebraska, 2004,3519.6\n",
      "Nebraska, 2005,3432\n",
      "Nebraska, 2006,3364.9\n",
      "Nebraska, 2007,3142.8\n",
      "Nebraska, 2008,2878.3\n",
      "Nevada, 2004,4210\n",
      "Nevada, 2005,4246\n",
      "Nevada, 2006,4099.6\n",
      "Nevada, 2007,3785.1\n",
      "Nevada, 2008,3456.4\n",
      "New 2004,2051.9\n",
      "New 2005,1839\n",
      "New 2006,2061.8\n",
      "New 2007,1968.6\n",
      "New 2008,2132.1\n",
      "New 2004,2433\n",
      "New 2005,2337\n",
      "New 2006,2278.4\n",
      "New 2007,2205.5\n",
      "New 2008,2293.4\n",
      "New 2004,4198.4\n",
      "New 2005,4132\n",
      "New 2006,3947.5\n",
      "New 2007,3846.7\n",
      "New 2008,3817.4\n",
      "New 2004,2192.5\n",
      "New 2005,2102\n",
      "New 2006,2063.2\n",
      "New 2007,1992.1\n",
      "New 2008,1993.7\n",
      "North 2004,4160.5\n",
      "North 2005,4080\n",
      "North 2006,4119.5\n",
      "North 2007,4101.8\n",
      "North 2008,4041.1\n",
      "North 2004,1963.4\n",
      "North 2005,2025\n",
      "North 2006,2088.6\n",
      "North 2007,1996.8\n",
      "North 2008,2016.3\n",
      "Ohio, 2004,3662.3\n",
      "Ohio, 2005,3668\n",
      "Ohio, 2006,3716.2\n",
      "Ohio, 2007,3461.6\n",
      "Ohio, 2008,3419.2\n",
      "Oklahoma, 2004,4242.1\n",
      "Oklahoma, 2005,4047\n",
      "Oklahoma, 2006,3625\n",
      "Oklahoma, 2007,3549.8\n",
      "Oklahoma, 2008,3456.6\n",
      "Oregon, 2004,4635.4\n",
      "Oregon, 2005,4402\n",
      "Oregon, 2006,3719.1\n",
      "Oregon, 2007,3530.1\n",
      "Oregon, 2008,3299.2\n",
      "Pennsylvania, 2004,2417.3\n",
      "Pennsylvania, 2005,2422\n",
      "Pennsylvania, 2006,2451\n",
      "Pennsylvania, 2007,2364.4\n",
      "Pennsylvania, 2008,2412.4\n",
      "Rhode 2004,2886\n",
      "Rhode 2005,2728\n",
      "Rhode 2006,2614.6\n",
      "Rhode 2007,2602.2\n",
      "Rhode 2008,2845\n",
      "South 2004,4536.9\n",
      "South 2005,4370\n",
      "South 2006,4277.1\n",
      "South 2007,4282.6\n",
      "South 2008,4241.2\n",
      "South 2004,1931.6\n",
      "South 2005,1767\n",
      "South 2006,1811.1\n",
      "South 2007,1774\n",
      "South 2008,1706.1\n",
      "Tennessee, 2004,4326.8\n",
      "Tennessee, 2005,4300\n",
      "Tennessee, 2006,4137.7\n",
      "Tennessee, 2007,4092.2\n",
      "Tennessee, 2008,4048.3\n",
      "Texas, 2004,4497.7\n",
      "Texas, 2005,4319\n",
      "Texas, 2006,4084.2\n",
      "Texas, 2007,4121.6\n",
      "Texas, 2008,3987\n",
      "Utah, 2004,4038.6\n",
      "Utah, 2005,3837\n",
      "Utah, 2006,3505.3\n",
      "Utah, 2007,3510.1\n",
      "Utah, 2008,3374\n",
      "Vermont, 2004,2343.6\n",
      "Vermont, 2005,2370\n",
      "Vermont, 2006,2368\n",
      "Vermont, 2007,2339\n",
      "Vermont, 2008,2559.8\n",
      "Virginia, 2004,2678.2\n",
      "Virginia, 2005,2649\n",
      "Virginia, 2006,2479.6\n",
      "Virginia, 2007,2480\n",
      "Virginia, 2008,2531.8\n",
      "Washington, 2004,4846.7\n",
      "Washington, 2005,4890\n",
      "Washington, 2006,4483.3\n",
      "Washington, 2007,4026\n",
      "Washington, 2008,3785\n",
      "West 2004,2555.8\n",
      "West 2005,2633\n",
      "West 2006,2639.9\n",
      "West 2007,2543.6\n",
      "West 2008,2554.4\n",
      "Wisconsin, 2004,2665.7\n",
      "Wisconsin, 2005,2669\n",
      "Wisconsin, 2006,2820\n",
      "Wisconsin, 2007,2842.8\n",
      "Wisconsin, 2008,2761.1\n",
      "Wyoming, 2004,3338.5\n",
      "Wyoming, 2005,3158\n",
      "Wyoming, 2006,2989.1\n",
      "Wyoming, 2007,2883.2\n",
      "Wyoming, 2008,2724.2\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "cat /tmp/crime-clean.txt | grep -v '^,$' | awk '/^[A-Z]/ {state = $4} !/^[A-Z]/ {print state, $0}'\n",
    "#cat crime-clean.txt | grep -v '^,$' | awk '/^[A-Z]/ {state = $4} !/^[A-Z]/ {print state, $0}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* On crime-clean.txt, the following script cleans the data. The following works assuming perfectly homogenous data (as the example on the Wrangler website is)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cat /tmp/crime-clean.txt |grep -v '^,$' | sed 's/,$//g; s/Reported crime in //; s/[0-9]*,//' | awk -F',' 'BEGIN {printf \"State, 2004, 2005, 2006, 2007, 2008\"} /^[A-Z]/ {print c; c=$0}  !/^[A-Z]/ {c=c\", \"$0;}  END {print c}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* On crime-unclean.txt the follow script perfroms the same cleaning as above, but allows incomplete information (e.g., some years may be missing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008\n",
      "Alabama,0,0,0,0,4029.3,0,0,3974.9,4081.9,\n",
      "Alaska,0,0,0,0,3370.9,3615,3582,3373.9,2928.3,\n",
      "Arizona,0,0,0,0,5073.3,4827,4741.6,4502.6,4087.3,\n",
      "Arkansas,0,0,0,0,4033.1,4068,4021.6,3945.5,3843.7,\n",
      "California,0,0,0,0,3423.9,3321,3175.2,3032.6,2940.3,\n",
      "Colorado,0,0,0,0,3918.5,4041,3441.8,2991.3,2856.7,\n",
      "Connecticut,0,0,0,0,2684.9,2579,2575,2470.6,2490.8,\n",
      "Delaware,0,0,0,0,3283.6,3118,3474.5,3427.1,3594.7,\n",
      "District of Columbia,0,0,0,0,4852.8,4490,4653.9,4916.3,5104.6,\n",
      "Florida,0,0,0,0,4182.5,4013,3986.2,4088.8,4140.6,\n",
      "Georgia,0,0,0,0,4223.5,4145,3928.8,3893.1,3996.6,\n",
      "Hawaii,0,0,0,0,4795.5,4800,4219.9,4119.3,3566.5,\n",
      "Idaho,0,0,0,0,2781,2697,2386.9,2264.2,2116.5,\n",
      "Illinois,0,0,0,0,3174.1,3092,3019.6,2935.8,2932.6,\n",
      "Indiana,0,0,0,0,3403.6,3460,3464.3,3386.5,3339.6,\n",
      "Iowa,0,0,0,0,2904.8,2845,2870.3,2648.6,2440.5,\n",
      "Kansas,0,0,0,0,4015.5,3806,3858.5,3693.8,3397,\n",
      "Kentucky,0,0,0,0,2540.2,2531,2621.9,2524.6,2677.1,\n",
      "Louisiana,0,0,0,0,4419.1,3696,4088.5,4196.1,3880.2,\n",
      "Maine,0,0,0,0,2413.7,2419,2546.1,2448.3,2463.7,\n",
      "Maryland,0,0,0,0,3640.7,3551,3481.2,3431.5,3516,\n",
      "Massachusetts,0,0,0,0,2468.2,2358,2396,2399.2,2402,\n",
      "Michigan,0,0,0,0,3066.1,3098,3226,3057.8,2945.7,\n",
      "Minnesota,0,0,0,0,3041.6,3088,3088.8,3045,2858.1,\n",
      "Mississippi,0,0,0,0,3481.1,3274,3213,3137.8,2941.7,\n",
      "Missouri,0,0,0,0,3900.1,3929,3828.4,3828.2,3663.6,\n",
      "Montana,0,0,0,0,2936.1,3146,2863.4,2863.6,2720.9,\n",
      "Nebraska,0,0,0,0,3519.6,3432,3364.9,3142.8,2878.3,\n",
      "Nevada,0,0,0,0,4210,4246,4099.6,3785.1,3456.4,\n",
      "New Hampshire,0,0,0,0,2051.9,1839,2061.8,1968.6,2132.1,\n",
      "New Jersey,0,0,0,0,2433,2337,2278.4,2205.5,2293.4,\n",
      "New Mexico,0,0,0,0,4198.4,4132,3947.5,3846.7,3817.4,\n",
      "New York,0,0,0,0,2192.5,2102,2063.2,1992.1,1993.7,\n",
      "North Carolina,0,0,0,0,4160.5,4080,4119.5,4101.8,4041.1,\n",
      "North Dakota,0,0,0,0,1963.4,2025,2088.6,1996.8,2016.3,\n",
      "Ohio,0,0,0,0,3662.3,3668,3716.2,3461.6,3419.2,\n",
      "Oklahoma,0,0,0,0,4242.1,4047,3625,3549.8,3456.6,\n",
      "Oregon,0,0,0,0,4635.4,4402,3719.1,3530.1,3299.2,\n",
      "Pennsylvania,0,0,0,0,2417.3,2422,2451,2364.4,2412.4,\n",
      "Rhode Island,0,0,0,0,2886,2728,2614.6,2602.2,2845,\n",
      "South Carolina,0,0,0,0,4536.9,4370,4277.1,4282.6,4241.2,\n",
      "South Dakota,0,0,0,0,1931.6,1767,1811.1,1774,1706.1,\n",
      "Tennessee,0,0,0,0,4326.8,4300,4137.7,4092.2,4048.3,\n",
      "Texas,0,0,0,0,4497.7,4319,4084.2,4121.6,3987,\n",
      "Utah,0,0,0,0,4038.6,3837,3505.3,3510.1,3374,\n",
      "Vermont,0,0,0,0,2343.6,2370,2368,2339,2559.8,\n",
      "Virginia,0,0,0,0,2678.2,2649,2479.6,2480,2531.8,\n",
      "Washington,0,0,0,0,4846.7,4890,4483.3,4026,3785,\n",
      "West Virginia,0,0,0,0,2555.8,2633,2639.9,2543.6,2554.4,\n",
      "Wisconsin,0,0,0,0,2665.7,2669,2820,2842.8,2761.1,\n",
      "Wyoming,0,0,0,0,3338.5,3158,2989.1,2883.2,2724.2,\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cat /tmp/crime_unclean.txt | grep -v '^,$' | sed 's/Reported crime in //;' | \n",
    "        awk -F',' 'BEGIN {printf \"State, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008\\n\"} /^[A-Z]/ || /^$/ { if(state) { printf(state); for(i = 2000; i <= 2008; i++) { if(array[i]) {printf(\"%s,\", array[i])} else {printf(\"0,\")} }; printf(\"\\n\");} state=$0; delete array} !/^[A-Z]/ {array[$1] = $2}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provided the last example to show how powerful awk can be. However if you need to write a long command like this, you may be better off using a proper scripting language like perl or python!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally, the tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use the awk \"split\" function and \"for loop\" constructs on World Cup data, to again generate output as follows, i.e., each line in the output contains a country, a year, and the position of the county in that year (if within top 4).\n",
    "\n",
    "        BRA, 1962, 1\n",
    "        BRA, 1970, 1\n",
    "        BRA, 1994, 1\n",
    "        BRA, 2002, 1\n",
    "        BRA, 1958, 1\n",
    "        BRA, 1998, 2\n",
    "        BRA, 1950, 2\n",
    "        ...\n",
    "\n",
    "* Start with the given script that cleans up the data a little bit.\n",
    "* No need to re-answer the questions in the Wrangler section, but recompute them to ensure your answers are consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ib.open(\"worldcup.txt\") as f:\n",
    "    world_cup=f.read()\n",
    "    \n",
    "open('/tmp/worldcup.txt','w').write(world_cup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! Team !! Titles !! Runners-up !! Third place !! Fourth place !! Top 4 <br/> finishes\n",
      "|-\n",
      "BRA\n",
      "|5 (1958, 1962, 1970, 1994, 2002)\n",
      "|2 (1950, 1998)\n",
      "|2 (1938, 1978)\n",
      "|1 (1974)\n",
      "|10\n",
      "|-\n",
      "ITA\n",
      "|4 (1934, 1938, 1982, 2006)\n",
      "|2 (1970, 1994)\n",
      "|1 (1990)\n",
      "|1 (1978)\n",
      "|8\n",
      "|-\n",
      "GER\n",
      "|3 (1954, 1974, 1990)\n",
      "|4 (1966, 1982, 1986, 2002)\n",
      "|4 (1934, 1970, 2006, 2010)\n",
      "|1 (1958)\n",
      "|12\n",
      "|-\n",
      "ARG\n",
      "|2 (1978, 1986)\n",
      "|2 (1930, 1990)\n",
      "| —\n",
      "| —\n",
      "|4\n",
      "|-\n",
      "URU\n",
      "|2 (1930, 1950)\n",
      "| —\n",
      "| —\n",
      "|3 (1954, 1970, 2010)\n",
      "|5\n",
      "|-\n",
      "FRA\n",
      "|1 (1998)\n",
      "|1 (2006)\n",
      "|2 (1958, 1986)\n",
      "|1 (1982)\n",
      "|5\n",
      "|-\n",
      "ENG\n",
      "|1 (1966)\n",
      "| —\n",
      "| —\n",
      "|1 (1990)\n",
      "|2\n",
      "|-\n",
      "ESP\n",
      "|1 (2010)\n",
      "| —\n",
      "| —\n",
      "|1 (1950)\n",
      "|2 \n",
      "|-\n",
      "NED\n",
      "| —\n",
      "|3 (1974, 1978, 2010)\n",
      "| —\n",
      "|1 (1998)\n",
      "|4\n",
      "|-\n",
      "TCH\n",
      "| —\n",
      "|2 (1934, 1962)\n",
      "| —\n",
      "| —\n",
      "|2\n",
      "|-\n",
      "HUN\n",
      "| —\n",
      "|2 (1938, 1954)\n",
      "| —\n",
      "| —\n",
      "|2\n",
      "|-\n",
      "SWE\n",
      "| —\n",
      "|1 (1958)\n",
      "|2 (1950, 1994)\n",
      "|1 (1938)\n",
      "|4\n",
      "|-\n",
      "POL\n",
      "| —\n",
      "| —\n",
      "|2 (1974, 1982)\n",
      "| —\n",
      "|2\n",
      "|-\n",
      "AUT\n",
      "| —\n",
      "| —\n",
      "|1 (1954)\n",
      "|1 (1934)\n",
      "|2\n",
      "|-\n",
      "POR\n",
      "| —\n",
      "| —\n",
      "|1 (1966)\n",
      "|1 (2006)\n",
      "|2\n",
      "|-\n",
      "USA\n",
      "| —\n",
      "| —\n",
      "|1 (1930)\n",
      "| —\n",
      "|1\n",
      "|-\n",
      "CHI\n",
      "| —\n",
      "| —\n",
      "|1 (1962)\n",
      "| —\n",
      "|1\n",
      "|-\n",
      "CRO\n",
      "| —\n",
      "| —\n",
      "|1 (1998)\n",
      "| —\n",
      "|1\n",
      "|-\n",
      "TUR\n",
      "| —\n",
      "| —\n",
      "|1 (2002)\n",
      "| —\n",
      "|1\n",
      "|-\n",
      "YUG\n",
      "| —\n",
      "| —\n",
      "| —\n",
      "|2 (1930, 1962)\n",
      "|2\n",
      "|-\n",
      "URS\n",
      "| —\n",
      "| —\n",
      "| —\n",
      "|1 (1966)\n",
      "|1\n",
      "|-\n",
      "BEL\n",
      "| —\n",
      "| —\n",
      "| —\n",
      "|1 (1986)\n",
      "|1\n",
      "|-\n",
      "BUL\n",
      "| —\n",
      "| —\n",
      "| —\n",
      "|1 (1994)\n",
      "|1\n",
      "|-\n",
      "KOR\n",
      "| —\n",
      "| —\n",
      "| —\n",
      "|1 (2002)\n",
      "|1\n",
      "|-\n",
      ":<div id=\"1\">''<nowiki>*</nowiki> = hosts''\n",
      ":<div id=\"2\">''<nowiki>^</nowiki> = includes results ''</div>\n",
      ":<div id=\"3\">''<sup><nowiki>#</nowiki></sup> = ''<ref name=\"successor\"/></div>\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "cat /tmp/worldcup.txt| sed 's/\\[\\[\\([0-9]*\\)[^]]*\\]\\]/\\1/g; s/.*fb|\\([A-Za-z]*\\)}}/\\1/g; s/<sup><\\/sup>//g; s/|bgcolor[^|]*//g; s/|align=center[^|]*//g'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Questions\n",
    "\n",
    "1. From your experience, briefly discuss the pro and cons between using Data Wrangler as compared to lower levels tools like sed/awk?\n",
    "2. What additional operations would have made using Data Wrangler \"easier\"?\n",
    "\n",
    "#### Note\n",
    "While responding to markdown cells (as the one below), in case you struggle with formatting, just double click any of the markdown cells in the notebook to see how formatting is done. You may also consult the documentation [here](http://jupyter-notebook.readthedocs.io/en/latest/examples/Notebook/Working%20With%20Markdown%20Cells.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***\n",
    "### Add your response below:\n",
    "\n",
    "1.\n",
    "\n",
    "2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 3. Tableau\n",
    "***\n",
    "\n",
    "Finally, you will perform data exploration with Tableau.\n",
    "\n",
    "1. Go to the [Tableau Website](https://www.tableau.com/), and download a demo version of Tableau.\n",
    "\n",
    "    * Tableau gives students a 1 year free license, which can be requested [here](http://www.tableau.com/freeforstudents).\n",
    "\n",
    "2. Connect Tableau to the the OnTime database, hosted on a postgreSQL server we set up for the course.\n",
    "To do so, create a new Tableau workbook.\n",
    "In the workbook, goto data and add a new data source using the credentials below:\n",
    "\n",
    "    * Type: PostgreSQL (you may have to download an additional driver for postgreSQL, [here](https://www.tableau.com/support/drivers)) (only if you can't connect)\n",
    "    * Hostname: pg-001.db.gcloud.instabase.com\n",
    "    * Username: columbia\n",
    "    * Password: B%38Mt5W@M*QU?Ar\n",
    "    * Database: db_fea10998_f88d_4b6e_8f90_a6cd73bac65c\n",
    "    * You should use the table called \"Ontime\".\n",
    "    \n",
    "3. Explore the dataset using Tableau.\n",
    "\n",
    "The aim of this assignment is to understand (1) which flights are the likeliest to be delayed (2) why they are delayed (3) what we could have missed in the data\n",
    "\n",
    "**(1) Which flights are delayed? (You're expected to answer any 2 of the 5 questions below)**\n",
    "- Long flights or short flights?\n",
    "- Which companies?\n",
    "- At what time of the day?\n",
    "- Which state are the most impacted?\n",
    "- Take California. Which cities are the most concerned? And how about NY state?\n",
    "\n",
    "**(2) Why are flights delayed? (You're expected to answer any 2  of the 6 questions below)**\n",
    "- What is the likeliest cause of delays?\n",
    "- Does that depend on the region?\n",
    "- Compare California and NY state\n",
    "- Compare Morning flights and evening flights\n",
    "- Compare weekends and rest of the week\n",
    "- Compare first week of dataset and last week\n",
    "\n",
    "**(3) what we could have missed in the data**\n",
    "- Find three quirky facts about flight delays. Anything goes, as long it involves at least one aggregate and one filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***\n",
    "### Add you response below:\n",
    "\n",
    "***\n",
    "1(a) (Question):\n",
    "\n",
    "1(a) (Response):\n",
    "\n",
    "***\n",
    "1(b) (Question):\n",
    "\n",
    "1(b) (Response):\n",
    "\n",
    "***\n",
    "2(a) (Question):\n",
    "\n",
    "2(a) (Response):\n",
    "\n",
    "***\n",
    "2(b) (Question):\n",
    "\n",
    "2(b) (Response):\n",
    "\n",
    "***\n",
    "3 (Response):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Submission\n",
    "\n",
    "* Got to 'File' and download this notebook as .ipynb\n",
    "* Rename it as **data\\_processing\\_[your uni].ipynb**\n",
    "* Then submit it on Instabase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
